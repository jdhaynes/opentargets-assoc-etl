# Open Targets Disease-Target Association ETL Pipeline
A simple extract-transform-load (ETL) pipeline to compute disease-target association metrics from publicly available EMBL-EBI Open Targets data.

## Description
This pipeline downloads disease-target association data, disease metadata and target metadata from the EMBL-EBI Open Targets public data hosted on the EMBL-EBI public FTP server in partitioned JSON. Downloads are performed in parallel using multiprocessing for enhanced performed. The data is then loaded into memory and manipulated as a DataFrame (`pandas`) to compute the median and top 3 scores for each disease-target association. The resulting data is saved as a JSON file to the data output directory.

For space, only the output files are committed to this repo.


## Installation
### Option 1: Python Virtual Environment
#### Requirements:
* Python 3.9
* Git
* Venv

Clone this repo and create a new Python virtual environment with `venv`. In the environment, install the required Python dependencies from `requirements.txt` with `pip`:

```{bash}
git clone https://github.com/jdhaynes/opentargets-assoc-etl
cd opentargets-assoc-etl
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
```

### Option 2: Docker Container
#### Requirements:
* Docker

Pull the image from Docker Hub:
```{bash}
docker pull jackhaynes/opentargets-assoc-etc
```

Alternatively, build the image from source in the repo root directory:

```{bash}
docker build -t jackhaynes/opentargets-assoc-etc .
```

## Executing Pipeline
### Option 1: Python Virtual Environment
Run the pipeline with the following command in the repo root directory:
```{bash}
python etl/run.py \
    --ftp-server ftp.ebi.ac.uk \
    --assoc-dir pub/databases/opentargets/platform/21.11/output/etl/json/evidence/sourceId=eva/ \
    --disease-dir pub/databases/opentargets/platform/21.11/output/etl/json/diseases/ \
    --target-dir pub/databases/opentargets/platform/21.11/output/etl/json/targets/ \
    --output-dir ./data2 \
    --processes 4
```

### Option 2: Docker Container
First, create a Docker volume to store data generated by the pipeline:

```{bash}
docker volume create opentargets-assoc-etl-vol
```

Secondly, run the pipeline by starting a new container and executing the main run script:

```{bash}
docker run --volume opentargets-assoc-etl-vol:/data jackhaynes/opentargets-assoc-etl \
    --ftp-server ftp.ebi.ac.uk \
    --assoc-dir pub/databases/opentargets/platform/21.11/output/etl/json/evidence/sourceId=eva/ \
    --disease-dir pub/databases/opentargets/platform/21.11/output/etl/json/diseases/ \
    --target-dir pub/databases/opentargets/platform/21.11/output/etl/json/targets/ \
    --output-dir /data \
    --processes 4
```

### Pipeline Arguments
The following arguments are required by the main Python script `run.py`:

| Argument      | Description                                                                 |
|---------------|-----------------------------------------------------------------------------|
| --ftp-server  | The address of the EMBL-EBI FTP server.                                     |
| --assoc-dir   | Directory on the FTP server containing disease-target association data.     |
| --disease-dir | Directory on the FTP server containing disease data.                        |
| --target-dir  | Directory on the FTP server containing target association data.             |
| --output-dir  | Directory on the local filesystem to store output data.                     |
| --processes   | Number of processes to use for tasks utilising multiprocessing (default 1). |